{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "6ffc88b7-aa1c-45a3-8b3b-ace353ce0d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-07 12:46:15--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.189.102, 64.233.189.100, 64.233.189.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.189.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v2uvr069vb200t44k2508vv01b4b38jj/1651927575000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-07 12:46:16--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v2uvr069vb200t44k2508vv01b4b38jj/1651927575000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   175MB/s    in 0.4s    \n",
            "\n",
            "2022-05-07 12:46:17 (175 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "8e801d58-002e-4aa1-ae82-aab6ed5d97cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "a648cd3c-8a77-44c9-be2a-07a1859d8046",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "d3c39e84-bb29-4c05-9757-aa205c728a41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 11s 13ms/step - loss: 5.9817 - accuracy: 0.0288\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.4391 - accuracy: 0.0368\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3711 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3226 - accuracy: 0.0399\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.2541 - accuracy: 0.0404\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.1888 - accuracy: 0.0404\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.1358 - accuracy: 0.0348\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.0890 - accuracy: 0.0444\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 5.0381 - accuracy: 0.0404\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 4.9761 - accuracy: 0.0540\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 4.9141 - accuracy: 0.0515\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 4.8384 - accuracy: 0.0555\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 4.7626 - accuracy: 0.0666\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 4.6804 - accuracy: 0.0681\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 4.5925 - accuracy: 0.0838\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 4.5049 - accuracy: 0.0908\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.4175 - accuracy: 0.0959\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 4.3372 - accuracy: 0.1070\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 4.2776 - accuracy: 0.1100\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.1972 - accuracy: 0.1251\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 4.1178 - accuracy: 0.1362\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 4.0432 - accuracy: 0.1443\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 3.9942 - accuracy: 0.1579\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 3.9258 - accuracy: 0.1680\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 3.8563 - accuracy: 0.1907\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 3.7869 - accuracy: 0.2059\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 3.7151 - accuracy: 0.2185\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.6577 - accuracy: 0.2296\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.5879 - accuracy: 0.2503\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.5089 - accuracy: 0.2634\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.4599 - accuracy: 0.2679\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.3972 - accuracy: 0.2790\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.3442 - accuracy: 0.2871\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.2854 - accuracy: 0.3017\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.2420 - accuracy: 0.3189\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.2366 - accuracy: 0.3189\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.1337 - accuracy: 0.3365\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.0621 - accuracy: 0.3602\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.0049 - accuracy: 0.3769\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.9411 - accuracy: 0.3915\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.8927 - accuracy: 0.4026\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.8685 - accuracy: 0.3971\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.8314 - accuracy: 0.4087\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.7706 - accuracy: 0.4213\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.7207 - accuracy: 0.4374\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.6872 - accuracy: 0.4425\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.6897 - accuracy: 0.4289\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.6257 - accuracy: 0.4506\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.5548 - accuracy: 0.4581\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.4963 - accuracy: 0.4813\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.4666 - accuracy: 0.4798\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.4369 - accuracy: 0.4828\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.3772 - accuracy: 0.4859\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.3318 - accuracy: 0.5035\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.2950 - accuracy: 0.5086\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.2593 - accuracy: 0.5156\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.2299 - accuracy: 0.5146\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1926 - accuracy: 0.5277\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1517 - accuracy: 0.5373\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1188 - accuracy: 0.5424\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0857 - accuracy: 0.5535\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0492 - accuracy: 0.5696\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0193 - accuracy: 0.5656\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0003 - accuracy: 0.5782\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.9717 - accuracy: 0.5812\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.9265 - accuracy: 0.5908\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8914 - accuracy: 0.6044\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8699 - accuracy: 0.6070\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.8431 - accuracy: 0.6145\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8095 - accuracy: 0.6236\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8036 - accuracy: 0.6145\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.7773 - accuracy: 0.6211\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.7327 - accuracy: 0.6347\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.7108 - accuracy: 0.6433\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6879 - accuracy: 0.6468\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6990 - accuracy: 0.6372\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6680 - accuracy: 0.6418\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6204 - accuracy: 0.6579\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5890 - accuracy: 0.6599\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5590 - accuracy: 0.6705\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5345 - accuracy: 0.6741\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5166 - accuracy: 0.6771\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.4854 - accuracy: 0.6852\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4703 - accuracy: 0.6847\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4468 - accuracy: 0.6917\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4340 - accuracy: 0.6917\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4135 - accuracy: 0.7023\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3853 - accuracy: 0.7064\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.3698 - accuracy: 0.7094\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.3451 - accuracy: 0.7144\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.3263 - accuracy: 0.7129\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3099 - accuracy: 0.7139\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2887 - accuracy: 0.7195\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2773 - accuracy: 0.7195\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2675 - accuracy: 0.7281\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2756 - accuracy: 0.7180\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2425 - accuracy: 0.7270\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2291 - accuracy: 0.7270\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2073 - accuracy: 0.7281\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.1846 - accuracy: 0.7336\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2673 - accuracy: 0.7124\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2406 - accuracy: 0.7250\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1838 - accuracy: 0.7245\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1474 - accuracy: 0.7341\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1216 - accuracy: 0.7407\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1383 - accuracy: 0.7346\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1050 - accuracy: 0.7452\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0744 - accuracy: 0.7523\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.0499 - accuracy: 0.7553\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0309 - accuracy: 0.7563\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0180 - accuracy: 0.7619\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0084 - accuracy: 0.7603\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0032 - accuracy: 0.7598\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9824 - accuracy: 0.7669\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9680 - accuracy: 0.7654\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9554 - accuracy: 0.7725\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9420 - accuracy: 0.7770\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9333 - accuracy: 0.7790\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9264 - accuracy: 0.7810\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9191 - accuracy: 0.7830\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9130 - accuracy: 0.7825\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9374 - accuracy: 0.7730\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 0.9112 - accuracy: 0.7846\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8954 - accuracy: 0.7846\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8673 - accuracy: 0.7931\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8683 - accuracy: 0.7941\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8519 - accuracy: 0.7972\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8403 - accuracy: 0.8012\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8308 - accuracy: 0.8063\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8185 - accuracy: 0.8073\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8208 - accuracy: 0.8007\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8025 - accuracy: 0.8052\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8001 - accuracy: 0.8118\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7920 - accuracy: 0.8113\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8593 - accuracy: 0.7921\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8137 - accuracy: 0.8002\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7932 - accuracy: 0.8113\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8815 - accuracy: 0.7775\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8612 - accuracy: 0.7805\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8101 - accuracy: 0.8037\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7786 - accuracy: 0.8083\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7553 - accuracy: 0.8169\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7378 - accuracy: 0.8239\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7229 - accuracy: 0.8219\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7263 - accuracy: 0.8189\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7153 - accuracy: 0.8214\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7029 - accuracy: 0.8300\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6917 - accuracy: 0.8340\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6851 - accuracy: 0.8325\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6808 - accuracy: 0.8335\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6800 - accuracy: 0.8320\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6681 - accuracy: 0.8391\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6929 - accuracy: 0.8274\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.6880 - accuracy: 0.8269\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 0.6800 - accuracy: 0.8290\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6640 - accuracy: 0.8385\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6499 - accuracy: 0.8370\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6584 - accuracy: 0.8385\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6299 - accuracy: 0.8461\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6171 - accuracy: 0.8471\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6181 - accuracy: 0.8461\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6254 - accuracy: 0.8416\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6151 - accuracy: 0.8476\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5989 - accuracy: 0.8542\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5870 - accuracy: 0.8547\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.5772 - accuracy: 0.8562\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5730 - accuracy: 0.8577\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5655 - accuracy: 0.8638\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5612 - accuracy: 0.8557\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5540 - accuracy: 0.8597\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5500 - accuracy: 0.8628\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5578 - accuracy: 0.8567\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 0.5601 - accuracy: 0.8602\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 0.5433 - accuracy: 0.8643\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 0.5350 - accuracy: 0.8648\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5317 - accuracy: 0.8678\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5273 - accuracy: 0.8693\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5356 - accuracy: 0.8643\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5255 - accuracy: 0.8633\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5222 - accuracy: 0.8688\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5119 - accuracy: 0.8729\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.5524 - accuracy: 0.8577\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5399 - accuracy: 0.8683\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5120 - accuracy: 0.8673\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5030 - accuracy: 0.8724\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4985 - accuracy: 0.8729\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4954 - accuracy: 0.8708\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4885 - accuracy: 0.8713\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4820 - accuracy: 0.8764\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4750 - accuracy: 0.8824\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.4744 - accuracy: 0.8819\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4972 - accuracy: 0.8678\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4822 - accuracy: 0.8729\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4686 - accuracy: 0.8764\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4620 - accuracy: 0.8799\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4672 - accuracy: 0.8739\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4692 - accuracy: 0.8799\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4536 - accuracy: 0.8799\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4584 - accuracy: 0.8774\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4391 - accuracy: 0.8829\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "f1bd1e48-74db-4465-8279-85bf47132811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "5a80c3ce-73a9-437d-915f-40168d3329d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills you return to sender a crazy world chiquitita song eyes didnt crazy eyes am eyes eyes believe believe eyes believe believe song song song song song before making making holler think do realized chiquitita cassandra strong do what could i found out that that that door that me quiet her more cassandra strong strong strong body music hear what us wanted but none of us would making chiquitita smiled new chiquitita eyes cassandra feel body song song song kind wanted new new realized bags wanted sender body am sender believe you think be am blue crazy eyes am eyes am\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}